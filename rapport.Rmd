---
title: "Is it raining tommorow ?"
author: "Théo LAZZARONI, Gautier POURSIN"
date: "23/04/2020"
output: 
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Dans ce projet, nous allons utiliser les données fournies par “Rain in Australia” accessible sur le
site Kaggle : https://www.kaggle.com/jsphyg/weather-dataset-rattle-package. Nous allons plus particulièrementnous intéressés à 3 villes : **Brisbane**, **Canberra** et **Adelaide**. Ces villes ont été choisies pour leur localisation. Adelaide est située à 1000km à l'ouest de Sydney, Canberra est elle située à 100km des côtes et à 350km de Sydney. Enfin, Brisbane est à 500km au nord de Sydney sur les côtes également. Ces choix nous permettent d'avoir des situations géographiques assez différentes mais où la pluie devrait être assez présente pour avoir des résultats plus probants.
**Le but du projet est de réussir via une étude des données à prédire le temps qu'il fera le lendemain** à partir des informations disponibles sur les jours précédents.

# Phase 1 : Prise en main des données

#  Question 1 :

## Importation des bibliothèques utiles :

```{r message=FALSE, warning=FALSE}
library(magrittr)
library(data.table)
library(tidyverse)
library(plotly)
library(rmarkdown)
library(ggplot2)
library(corrplot)
library(glm2)
```

## Téléchargements des données et création des dataframes :

Tout d'abord, on crée un dataframe contenant les données disponibles.

```{r}
data <- read.csv("weatherAUS.csv")
```

On crée ensuite différents dataframes pour chacune des trois villes étudiées. On remarque que certaines données sont manquantes, par exemple **la pression n'a pas été calculée à Penrith**.

```{r}
data.brisbane <-data[data$Location == "Brisbane", ]
data.canberra <-data[data$Location == "Canberra", ]
data.adelaide <-data[data$Location == "Adelaide", ]
```

On peut par exemple montrer les **3 premières lignes des données de Canberra **: 

```{r}
head(data.canberra, n=3L)
```

# Question 2 :

## Analyse descriptive des données :

Avant de commencer l'étude à proprement parler, on va d'abord décrire le dataframe que l'on utilise en décrivant les 23 variables disponibles :

- Date : jour de l'observation de l'ensemble des variables de la ligne, c'est une variable temporelle.
- Location : c'est l'endroit où les mesures de la ligne ont été réalisées, c'est une valeur qualitative.
- MinTemp : la température minimale de la journée en degrés, c'est une variable quantitative.
- MaxTemp : la température maximale de la journée en degrés, c'est une variable quantitative.
- Rainfall : la quantité de pluie tombée dans la journée en mm, c'est variable quantitative.
- Evaporation : la quantité d'eau évaporée dans la journée en mm, c'est une variable quantitative.
- Sunshine : le nombre d'heures d'ensoleillement dans la journée, c'est une variable quantitative.
- WindGustDir : la direction de la plus grosse rafale de vent de la journée, c'est une variable qualitative.
- WindGustSpeed : la vitesse de la plus grosse rafale de la journée en km/h, c'est une variable quantitative.
- WindDir9am : la direction du vent à 9h, c'est une variable qualitative.
- WindSpeed9am : la vitesse moyenne du vent lors des 10 minutes avant 9h, c'est une variable quantitative.
- WindDir3pm : la direction du vent à 15h, c'est une variable qualitative.
- WindSpeed3pm : la vitesse moyenne du vent lors des 10 minutes avant 15h, c'est une variable quantitative.
- Humidity9am : l'humidité à 9h en %, c'est une variable quantitative.
- Humidity3pm : l'humidité à 15h en %, c'est une varible quantitative.
- Pressure9am : Pression atmosphérique en hpa réduit au niveau de la mer à 9h, c'est une variable quantitative.
- Pressure3pm : Pression atmosphérique en hpa réduit au niveau de la mer à 15h, c'est une variable quantitative.
- Cloud9am : Fraction du ciel obscurci par les nuages en oktas (échelle à 8 mesures) à 9h, c'est une variable qualitative.
- Cloud3pm : Fraction du ciel obscurci par les nuages en oktas (échelle à 8 mesures) à 15h, c'est une variable qualitative.
- Temp9am : Température à 9h en °C, c'est une variable quantitative.
- Temp3pm : Température à 15h en °C, c'est une variable quantitative.
- RainToday : Booléen valant 1 si la précipitation des dernières 24h à 9h à dépasser 1mm et 0 sinon, c'est une valeur qualitative.
Enfin il y a deux variables que nous n'utiliserons que pour vérifier nos estimations : 

- RISK_MM : La quantité de pluie le jour suivant, c'est une mesure du risque de pluie.
- Rain_Tomorrow : La variable à trouver, pleuvra-t-il demain ? </br>

On remarque que certaines valeurs sont manquantes, par exemple :

```{r}
sum(is.na(data.adelaide$Pressure9am))
```

On va donc les remplacer par des valeurs choisies pour les variables continues en conséquence de cause. On propose deux raisonnements distincts :

- On **utilise une loi normale de mêmes paramètres que les lois étudiées**.
- On remplace les valeurs NA par **la moyenne des valeurs de la variable**.

Malheureusement, ces deux choix tendent à nous faire nous approcher d'une loi normale, puisqu'on surestime la population autour de la moyenne. Cependant, le faible nombre de valeurs manquantes ne va que très peu influencé l'allure des variables.

# Question 3 :

## Analyse descriptive des variables :

Dans notre étude, on considère que les variables quantitatives ne prenant que des valeurs entières et ayant un petit nombre de modalités (<20) sont dites discrètes. Ainsi les variables considérées comme discrètes dans ce dataframe sont Cloud9am, Cloud3pm auxquels nous allons ajouter les deux pourcentages d'humidités qui peuvent être considérés comme discrets.

### Les variables continues : 

Maintenant que l'on a décrit les différentes données présentes dans notre dataframe, on va étudié les différentes variables via des calculs de moyenne, fréquence, variance mais également des histogrammes, boxplots. Pour cela, on crée une fonction qui prend en arguments une colonne d'un dataframe et qui remplace les valeurs NA par la moyenne des valeurs non NA.

```{r message=FALSE, warning=FALSE}
etude_variable <- function(c, nom){
  m <- mean(c, na.rm = TRUE)           
  c[is.na(c)] <- m
  cat("La moyenne est de",mean(c)," et la variance est de", var(c),".\n")
  par(mfrow=c(2,2))
  hist(c,main=nom,cex.main=1)
  boxplot(c, main=nom,horizontal = TRUE,cex.main=1)
  hist((c-mean(c))/sqrt(var(c)), main="Loi centrée, réduite",cex.main=1)
  boxplot((c-mean(c))/sqrt(var(c)), main="Loi centrée, réduite",horizontal = TRUE,cex.main=1)
}
```

On obtient ainsi par exemple pour les températures minimales à Adelaide :

```{r}
#+ graph-ajust, fig.width = 5, fig.height = 4
etude_variable(data.adelaide$MinTemp, 'Température minimum à Adélaide')
```

On peut réaliser la même opération avec d'autres valeurs continues : 
```{r message=FALSE, warning=FALSE, eval=FALSE}
etude_variable(data.adelaide$MaxTemp, 'Température maximum à Adélaide')
etude_variable(data.brisbane$MinTemp, 'Température minimum à Brisbane')
etude_variable(data.canberra$MinTemp, 'Température minimum à Canberra')
etude_variable(data.canberra$Humidity3pm, 'Humidité à 15h à Canberra')
```

On remarque quasiment à chaque fois, qu'avec la décision de remplacer les valeurs NA, on s'approche de lois normales puisque lorsqu'on les centre et réduit, on à quasiment N(0,1).
Si on utilise l'autre manière de s'occuper des NA, on obtient : 

```{r message=FALSE,warning=FALSE}
etude_variable_norm <- function(c, nom){
  m <- mean(c, na.rm = TRUE)
  v <- var(c,na.rm = TRUE)
  c[is.na(c)] <- rnorm(1,m,v) 
  cat("La moyenne est de",mean(c)," et la variance est de", var(c),".\n")
  par(mfrow=c(2,2))
  hist(c,main=nom,cex.main=1)
  boxplot(c, main=nom,horizontal = TRUE,cex.main=1)
  hist((c-mean(c))/sqrt(var(c)), main="Loi centrée, réduite",cex.main=1)
  boxplot((c-mean(c))/sqrt(var(c)), main="Loi centrée, réduite",horizontal = TRUE,cex.main=1)
}
```

En testant avec la même variable (MinTemp à Adelaide), on obtient :

```{r}
etude_variable_norm(data.adelaide$MinTemp, 'Température minimum à Adélaide')
```

Cependant, il existe certaines variables pour lesquelles la loi normale centrée réduite ne paraît pas être une bonne solution : 

```{r}
etude_variable(data.brisbane$Evaporation, 'Evaporation à Brisbane')
etude_variable(data.brisbane$Rainfall, 'Quantité de pluie à Brisbane')
etude_variable(data.brisbane$WindSpeed9am, 'Vitesse du vent à 9h à Brisbane')
```

### Les variables discrètes : 

En plus de simplement étudier les variables discrètes, on peut déjà commencer à s'intéresser aux facteurs influençants les jours de pluie :
```{r message=FALSE, warning=FALSE}
data.brisbane.rain = subset(data.brisbane, data.brisbane$RainToday =='Yes')
```
On peut ainsi étudier les variables Cloud3pm et Cloud9am :

```{r}
par(mfrow=c(2,2))
hist(data.brisbane$Cloud9am, breaks = 8, main= "Fraction du ciel obscurci à 9h à Adelaide")
hist(data.brisbane.rain$Cloud9am, breaks = 8, main= "Fraction du ciel obscurci à 9h à Adelaide les jours de pluie")
hist(data.brisbane$Cloud3pm, breaks = 8, main= "Fraction du ciel obscurci à 15h à Adelaide")
hist(data.brisbane.rain$Cloud3pm, breaks = 8, main= "Fraction du ciel obscurci à 15h à Adelaide les jours de pluie")
```

Ou les variables Humidity9am et Humidity3pm :

```{r}
par(mfrow=c(2,2))
hist(data.brisbane$Humidity9am, breaks = 8, main= "Humidité à 9h à Adelaide")
hist(data.brisbane.rain$Humidity9am, breaks = 8, main= "Humidité à 9h à Adelaide les jours de pluie")
hist(data.brisbane$Humidity3pm, breaks = 8, main= "Humidité à 15h à Adelaide")
hist(data.brisbane.rain$Humidity3pm, breaks = 8, main= "Humidité à 15h à Adelaide les jours de pluie")
```


### Les variables catégorielles/qualitatives :

Pour ces variables (WindGustDir, RainToday), on va se contenter d'afficher des tableaux de fréquence pour les variables.


```{r}
par(mfrow=c(2,2))
barplot(table(data.adelaide$WindGustDir),main = "Direction du vent à Adelaide",cex.names = .5)
barplot(table(data.adelaide$RainToday),main="Pluie à Adelaide")
barplot(table(data.brisbane.rain$WindGustDir),main = "Direction du vent jour de pluie Brisbane")
barplot(table(data.brisbane$WindGustDir),main = "Direction du vent Brisbane")
```

# Question 4 :

## Analyse bivariée des variables :

Pour réaliser une analyse bivariée de deux variables continues, on se propose de calculer la covariance, la corrélation et de tracer le Scatter associé.

```{r}
data.adelaide$MaxTemp[is.na(data.adelaide$MaxTemp)]<-mean(data.adelaide$MaxTemp,na.rm = TRUE)
data.adelaide$MinTemp[is.na(data.adelaide$MinTemp)]<-mean(data.adelaide$MinTemp,na.rm = TRUE)
data.brisbane$Cloud9am[is.na(data.brisbane$Cloud9am)]<- 4
data.brisbane$Temp9am[is.na(data.brisbane$Temp9am)]<-mean(data.brisbane$Temp9am,na.rm = TRUE)

cov(data.adelaide$MaxTemp,data.adelaide$MinTemp)
cor(data.adelaide$MaxTemp,data.adelaide$MinTemp)
par(mfrow=c(2,2))
smoothScatter(data.adelaide$MinTemp,data.adelaide$MaxTemp,xlab = 'Température min. Adelaide',ylab = 'Température max. Adelaide')
smoothScatter(data.adelaide$MaxTemp,data.adelaide$Evaporation,xlab ='Température max. Adelaide',ylab = 'Evaporation à Adelaide' )
smoothScatter(data.adelaide$RainToday,data.adelaide$MaxTemp,xlab = 'Pluie à Adelaide', ylab = 'Température max. à Adelaide')
smoothScatter(data.brisbane$MaxTemp,data.brisbane$Rainfall,xlab = 'Température max.à Brisbane', ylab = 'Quantité de pluie à Brisbane')
```
Noter l'allure du Scatter entre la pluie et la température maximale. 

On peut également mettre en évidence des corrélations entre les différentes variables numériques : 

```{r}

change_na_values <- function(c){
  m <- mean(c, na.rm = TRUE)
  c[is.na(c)] <- m
  return (c);
}

correlation_dataframe <- function(d){
  d.numeric = d[,c(3,4,5,6,7,9,12,13,14,15,16,17,20,21)]
  d.numeric = sapply(d.numeric, change_na_values)
  M <- cor(d.numeric)
  corrplot(M, method = "pie")
  }

correlation_dataframe(data.adelaide)
correlation_dataframe(data.brisbane)
correlation_dataframe(data.canberra)
```

# Question 5 :

## Covariance entre variable continue et variable discrète :

Pour calculer la covariance entre une variable continue et une variable discrète, il y a 2 possibilités: soit on essaye de rendre continue la variable discrète en appliquant ensuite un test du $\chi_2$. Si le test s'avère être bon, alors notre approximation n'est pas abérante et donc on peut ensuite calculer la covariance entre 2 variables continues. Sinon, on peut aussi discrétiser notre variable continue. C'est à dire que l'on sélectionne toutes les valeurs de la variable discrète puis on prend les valeurs de la variable continue pour chaque $x_i$. On peut alors calculer la covariance entre 2 variables discrètes.
On préférera ici utiliser la 2ème méthode. On va regarder l'évolution de la dispersion, de la moyenne ou de la variance selon un conditonnement de la variable catégorielle.
Par exemple : 

```{r}
data.brisbane.cloud9pm.0 = subset(data.brisbane, data.brisbane$Cloud9am ==0)
data.brisbane.cloud9pm.8 = subset(data.brisbane, data.brisbane$Cloud9am ==8)
mean(data.brisbane.cloud9pm.0$MaxTemp)
mean(data.brisbane.cloud9pm.8$MaxTemp, na.rm = TRUE)
par(mfrow=c(1,2))
boxplot(data.brisbane.cloud9pm.0$MaxTemp, main = 'Température Max pour ciel dégagé')
boxplot(data.brisbane.cloud9pm.8$MaxTemp, main = 'Température Max pour ciel rempli de nuages')

```
On voit bien avec cette étude la corrélation entre la température maximum et la proportion de nuages dans le ciel, plus il y a de nuages, moins il fait chaud dans la journée.

# Question 6 :

## Covariance entre deux variables qualitatives : 

Après des recherches sur Internet, nous avons trouvé ce qui suit : Lorsqu’on étudie simultanément deux variables qualitatives, il est commode de présenter les
données sous forme d’une table de contingence, synthèse des observations selon les modalités des
variables qu’elles ont présentées.
À partir de cette table, on définit la notion de profil, dont on se sert pour réaliser un diagramme
de profils faisant bien apparaître la liaison entre les deux variables, lorsqu’il en existe une.
Pour quantifier cette liaison, l’indicateur fondamental est le khi-deux. Toutefois, comme il n’est
pas d’usage commode dans la pratique, on introduit encore les indicateurs phi-deux, T de Tschuprow
et C de Cramer, liés au khi-deux. Les deux derniers sont compris entre 0 et 1, et sont d’autant
plus grands que la liaison est forte, ce qui facilite leur interprétation.



# Phase 2 : Modélisation des lois 

Dans la suite de la modélisation, on ne prendra plus en compte les variables Evaporation, Sunshine, Cloud9am, Cloud3pm car elles possèdent un nombre trop important de valeurs manquantes. On remarque aisément que les données utilisées sont des séries temporelles et par conséquent les observations sont donc fortement corréler entre elles. De ce fait, on ne peut pas dire que les données forment des échantillons i.i.d. Pour remédier à ce problème, on réalise le sous-échantillonnage suivant : on réalise une distribution mois par mois de chaque ville (par exemple : trouver la loi de la température à 9h à Brisbane, au mois de Septembre). On négligera tout impact du temps sur les données (réchauffement climatique etc.).

# Question 1 :

Pour réaliser le sous-échantillonnage désiré, on réalise 12 échantillons prenant 5 jours aléatoires par mois chaque année de l'échantillon. On devra également s'assurer que les observations sont distantes d'au moins 3 jours. Pour cela, on crée une fonction random_day qui nous renvoie une string correspondant à un jour entre m et n. Ensuite, la fonction selection_data s'occupe de faire les tirages (quasi-aléatoires) des jours sélectionnés. En réalité, il choisit un jour entre 1 et 3, 6 et 9, 12 et 15, 18 et 20, 23 et 27. C'est le tirage le plus aléatoire que nous ayons réussi à implémenter pour satisfaire le condition de 3 jours d'écart minimum entre les jours du tirage.

```{r}
random_day <- function(m,n){
  valeur = sample(m:n,1)
  if (valeur<10){
    return (paste("0",valeur,sep=""));
  }
  else return (paste0(valeur));
}

selection_data <- function(data,mois)
{
  random = random_day(1,3)
  retour = subset(data,data$Date == paste(2008,mois,random,sep="-"))
  day = c(random_day(6,9),random_day(12,15),random_day(18,20),random_day(23,26))
    for (j in day){
      retour <- rbind(retour,subset(data,data$Date == paste(2008,mois,j,sep="-"))) }
  year = c(2007,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018)
  for (i in year){
    day = c(random_day(1,3),random_day(6,9),random_day(12,15),random_day(18,20),random_day(23,26))
    for (j in day){
      retour <- rbind(retour,subset(data,data$Date == paste(i,mois,j,sep="-")))    
    }
  } 
  return (retour);  
}

data.brisbane.janvier = selection_data(data.brisbane,"01")
data.brisbane.fevrier = selection_data(data.brisbane,"02")
data.brisbane.mars = selection_data(data.brisbane,"03")
data.brisbane.avril= selection_data(data.brisbane,"04")
data.brisbane.juin = selection_data(data.brisbane,"06")
data.brisbane.fevrier = selection_data(data.brisbane,"02")
data.adelaide.juin = selection_data(data.adelaide,"06")
data.canberra.juin = selection_data(data.canberra,"06")
data.brisbane.juillet = selection_data(data.brisbane,"07")
data.brisbane.aout = selection_data(data.brisbane,"08")

```

# Question 2 : 

Une fois l'échantillonnage réalisé, on s'intéresse ensuite à trouver une distribution paramétrique pour les différentes variables (normal, gamma ...). On trace d'abord les histogrammes et histogrammes lissés des variables pour ensuite utiliser différents tests (comme Shapiro ou Kolmogorov-Smirnov) afin de trouver la distribution adéquate. On prendra ici $\alpha = 10\%$ en valeur seuil minimum de la p-value pour les tests de Shapiro et $\alpha = 20\%$ pour le test du $\chi^2$ avec une loi gamma, afin de garder l'hypothèse choisie, sinon on rejette l'hypothèse. On écrit donc une fonction etude_variable_mois qui réalise ces opérations et qui renvoie la distribution paramétrique le plus probable. On note qu'ici on remplace les NA values par la moyenne des valeurs non nulles.

```{r, eval=FALSE}
etude_variable_mois <- function(c){
  for (i in c(3,4,5,9,12,13,14,15,16,17,20,21)){
  m <- mean(c[,i], na.rm = TRUE)           
  c[,i][is.na(c[,i])] <- m
  v <- var(c[,i])
  cat("La moyenne est de",mean(c)," et la variance est de", var(c),".\n")
  par(mfrow=c(2,2))
  hist(c[,i],main=names(c[,i]),cex.main=1)
  plot(x = density(c[,i]),main = names(c[,i]), cex.main=1)
  if(shapiro.test(c[,i])$p.value>0.1){
    cat(names(c)[i],"suit une loi normale\n")
  }
  else 
    if(chisq.test(c[,i],rgamma(length(c[,i]),m*m/v,v/m))$p.value>0.2){
    cat(names(c)[i],"suit une loi gamma\n")
    }
  
  else cat(names(c)[i],"Modèle indécis\n")
  }
}

```
On remarque après quelques tests que la frontière entre une loi normale et une loi gamma est fine pour certains paramètres. Par exemple, MaxTemp à Brisbane oscille entre loi normale et loi gamma. On va donc implémenter une deuxième fonction qui va renvoyer sur tous les mois dans une ville, la distribution paramétrique la plus plausible. Ainsi, on prendra la même loi pour chaque mois de l'année afin de pouvoir comparer les EMV du maximum de vraisemblance de mêmes paramètres (cf. question 4)).


```{r,warning=FALSE,message=FALSE,eval=FALSE}
etude_variable_mois_solo <- function(c,i){
  m <- mean(c[,i], na.rm = TRUE)           
  c[,i][is.na(c[,i])] <- m
  v <- var(c[,i])
   ### par(mfrow=c(2,2))
  ###hist(c[,i],main=names(c[,i]),cex.main=1)
  ###plot(x = density(c[,i]),main = names(c[,i]), cex.main=1)
  if(shapiro.test(c[,i])$p.value>0.1){
    return (0);
  }
  else 
    if(chisq.test(c[,i],rgamma(length(c[,i]),m*m/v,v/m))$p.value>0.2){
    return (1);
    }
  
  else return (2);
  }

distribution_parametrique <- function(c,n){
  gamma = 0
  normale = 0
  for (i in c("01","02","03","04","05","06","07","08","09","10","11","12")){
    data <- selection_data(c,i)
      if (etude_variable_mois_solo(data,n)==0){
        normale = normale + 1
      }
    if (etude_variable_mois_solo(data,n)==1){
      gamma = gamma + 1
    }
  }
  if (gamma<normale){
    return ("normale")
  }
  return ("gamma")
}

distribution_parametrique_ville <- function(c){
  cat("Voici les distributions paramétriques pour la ville ", c$Location[1],"\n" )
  for (n in c(3,4,5,9,12,13,14,15,16,17,20,21)){
    cat(names(c)[n]," : ",distribution_parametrique(c,n),"\n")
  }
}

distribution_parametrique_ville(data.adelaide)
distribution_parametrique_ville(data.brisbane)
distribution_parametrique_ville(data.canberra)

```

On prendra finalement la répartition suivante : 

MinTemp  :  normale 
MaxTemp  :  normale 
Rainfall  :  gamma 
WindGustSpeed  :  gamma 
WindSpeed9am  :  gamma 
WindSpeed3pm  :  gamma 
Humidity9am  :  normale 
Humidity3pm  :  normale 
Pressure9am  :  normale 
Pressure3pm  :  normale 
Temp9am  :  normale 
Temp3pm  :  normale 

Elle a été choisie en pernant la "moyenne" des trois villes et en supposant que les variables 3pm et 9am d'une même grandeur doivent suivre une loi identique.

# Question 3 : 

On s'intéresse maintenant à l'écriture de la vraisemblance des différentes variables en utilisant les distributions paramétriques supposées à la question 2).

## Loi Normale :


Une loi normale $\mathcal{N}(\mu,\sigma^2)$ a pour fonction de densité : $f(x|\mu,\sigma^2)= \frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{2\sigma^2})$.
On en déduit la vraisemblance $L(x_1,...,x_n|\mu,\sigma^2)=(\frac{1}{2\pi\sigma^2})^{n/2}exp(-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2})$.

On peut donc calculer les EMV de $\mu$ et $\sigma$ en dérivant selon les deux paramètres. On obtient finalement $\bar{\mu} = \bar{x} = \frac{1}{n}\sum_{i=1}^nx_i$ et $\bar{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2$ respectivement les moyennes et variances empiriques.

On sait que les intervalles de confiance des deux paramètres sont les suivants : $\bar{X} - \alpha/2\frac{S}{\sqrt{n-1}} < \mu < \bar{X} + \alpha/2 \frac{S}{\sqrt{n-1}}$ avec $S=\sqrt{\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2}$.

De même, $\frac{nS^2}{1-\alpha/2}<\sigma^2<\frac{nS^2}{\alpha/2}$.



## Loi Gamma : 

Une loi gamme $\Gamma(a,b)$ a pour fonction de densité : $f(x|a,b)=\frac{b^a}{\Gamma(a)}x^{a-1}e^{-bx}$.
On en déduit la log-vraisemblance $\mathcal{L}(x_1,...,x_n|a,b)= na \log(b) - n\log(\Gamma(a)) + (a-1)\sum_{i=1}^n\log(x_i) -b\sum_{i=1}^nx_i$.

En dérivant selon b, on obtient un estimateur du paramètre $\bar{b}=\frac{a}{\bar{x}}$ avec $\bar{x}$ la moyenne empirique. De même, on trouve $\bar{a}=\log(b\sum_{i=1}^nx_i)$, cependant nous ne voyons pas comment aller plus loin dans le calcul étant donnée la realtion entre a et b même. On a donc décidé de prendre un simple estimateur pour continuer dans les questions suivantes. On peut estimer $a$ et $b$ grâce à la méthode des moments en sachant que $\mathbb{E}(X)=\frac{a}{b}$ et $\mathbb{V}(X)=\frac{a}{b^2}$. Finalement, on déduit de la méthode des moments : $a = \frac{\mathbb{E}(X)^2}{\mathbb{V}(X)}$ et $b=\frac{\mathbb{E}(X)}{\mathbb{V}(X)}$.

Cependant, ces deux valeurs données grâce à la méthode des moments ne nous donnent pas un EMV, on va donc tenter de calculer directement le maximum de la fonction pour obtenir les estimateurs de $a$ et $b$. 


# Question 4 :

Une fois les EMV théoriques calculés, on a juste à tracer leur évolution en fonction des mois dans une même ville

```{r}
evolution_param_normale <- function(c,n){
  m = c()
  v = c()
   for (i in c("01","02","03","04","05","06","07","08","09","10","11","12")){
     data <- selection_data(c,i)
     m <- c(m,mean(data[,n]))
     v = c(v,var(data[,n])) 
   }
  par(mfrow=c(2,2))
  plot(c("01","02","03","04","05","06","07","08","09","10","11","12"),m,ylab="Estimateur de la moyenne",xlab="Mois",main=names(c)[n])
  plot(c("01","02","03","04","05","06","07","08","09","10","11","12"),v,ylab="Estimateur de la variance",xlab="Mois",main=names(c)[n])  
}

evolution_param_normale(data.brisbane,3)
evolution_param_normale(data.canberra,3)
evolution_param_normale(data.canberra,4)
evolution_param_normale(data.brisbane,17)
evolution_param_normale(data.brisbane,20)
evolution_param_normale(data.brisbane,21)

evolution_param_normale(data.brisbane,14)
evolution_param_normale(data.brisbane,16)

evolution_param_gamma <- function(c,n){
  a = c()
  b = c()
   for (i in c("01","02","03","04","05","06","07","08","09","10","11","12")){
     data <- selection_data(c,i)
     a <- c(a,mean(data[,n])**2/(var(data[,n])))
     b = c(b,mean(data[,n])/var(data[,n])) 
   }

  par(mfrow=c(2,2))
  plot(c("01","02","03","04","05","06","07","08","09","10","11","12"),a,ylab="Estimateur de a ",xlab="Mois",main=names(c)[n])
  plot(c("01","02","03","04","05","06","07","08","09","10","11","12"),b,ylab="Estimateur de b",xlab="Mois",main=names(c)[n])  
}

evolution_param_gamma(data.brisbane,13)
evolution_param_gamma(data.adelaide,12)
evolution_param_gamma(data.canberra,13)

###mylogit = glm(RainTomorrow ~ 0, data.adelaide,family=binomial)
```

On remarque un comportement sinusoïdal concernant les variables suivant une distribution normale. C'est évidemment un comportement prévisible et qui met en avant les différences entre été et hiver. Noter aussi pour la variance que l'on a de plus gros écarts l'hiver pour la température min et inversement pour le max. On a sensiblement les mêmes variations (été/hiver) et 
Pour la loi gamma on dirait plutôt du stationnaire avec du bruit. 

# Question 5-6 : 

Pour vérifier les suppositions trouvées à la question précédente, on peut proposer différents tests afin de comparer la non-stationnarité des valeurs (plus spécialement de la moyenne pour les valeurs suivant une loi normale). On pose le test suivant : 

$H_0$ : Les moyennes par mois pour les variables normales suivent une loi normale, $H_1$ : Les moyennes par mois pour les variables normales ne suivent pas une loi normale. On vérifie ce résultat avec un test de Shapiro.

```{r}
evolution_param_normale_array <- function(c,n){
  m = c()
  v = c()
   for (i in c("01","02","03","04","05","06","07","08","09","10","11","12")){
     data <- selection_data(c,i)
     m <- c(m,mean(data[,n]))
     v = c(v,var(data[,n])) 
   }
return (shapiro.test(m)$p.value) 
}

evolution_param_normale_array(data.canberra,4)
evolution_param_normale_array(data.brisbane,17)
evolution_param_normale_array(data.brisbane,20)
evolution_param_normale_array(data.brisbane,21)
```
 
 On remarque qu'on retrouve toujours une p-value très haute et on valide donc l'hypothèse $H_0$. Ce résultat nous conforte dans l'idée que les paramètres ne sont pas constants et varient avec les saisons.


# Phase 3 : Prédiction de pluie

Maintenant que nous avons déterminé les différentes lois suivies par les variables ainsi que l'évolution des différents paramètres statistiques, nous allons désormais nous recentrer sur un modèle prédictif pour Adelaide, Brisbane et Canberra.

# Question 1 :

Pour commencer, on s'intéresse à la probabilité de pluie le lendemain mois par mois. Pour cela, on implémente une fonction qui renvoie les données d'une ville sur les 10 ans pour 1 mois et on calcule ensuite la probabilité de pluie le lendemain durant le mois.

```{r}
select_data_month <- function(d,mois,nb_jour){
  retour = data.frame()
  for(y in c(2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018)){
    for (day in nb_jour){
        retour <- rbind(retour,subset(d, d$Date==paste(y,mois,day,sep="-")))
    }
  }
  return (retour)
}
nb_jour=c("01","02","03","04","05","06","07","08","09","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26",
          "27","28","29","30","31")


raintomorrow_month <- function(d,nb_jour){
  array = c()
  for (mois in c("01","02","03","04","05","06","07","08","09","10","11","12")){
    compteur = 0 
    data_mois = select_data_month(d,mois,nb_jour)
    for (i in seq(1,length(data_mois$RainTomorrow))){
      if(data_mois$RainTomorrow[i] == "Yes") { 
        compteur = compteur+ 1 
        }
    }
    array = rbind(array,c(compteur/length(data_mois$RainTomorrow)))
  }
  plot(array,xlab="Mois",ylab="Probabilité de Pluie")
  return(array)
}
par(mfrow=c(2,3))
raintomorrow_month(data.canberra,nb_jour)
raintomorrow_month(data.adelaide,nb_jour)
raintomorrow_month(data.brisbane,nb_jour)
shapiro.test(raintomorrow_month(data.canberra,nb_jour))
shapiro.test(raintomorrow_month(data.adelaide,nb_jour))
shapiro.test(raintomorrow_month(data.brisbane,nb_jour))
```

On voit que pour nos 3 villes, le modèle statistique associé à la probabilité de pluie par mois le lendemain est une loi normale. On a déjà calculé les E.M.V d'une loi normale ainsi que les intervalles de confiance associés aux paramètres.

# Question 2 : 

```{r}
test_5percent_raintomorrow <- function(d,nb_jour,mois){
    compteur = 0 
    data_mois = select_data_month(d,mois,nb_jour)
    for (i in seq(1,length(data_mois$RainTomorrow))){
      if(data_mois$RainTomorrow[i] == "Yes") { 
        compteur = compteur+ 1 
      }
    }
    print(compteur)
    return(prop.test(compteur,p=0.05,n=length(data_mois$RainTomorrow),correct=FALSE)$p.value)
}
a = test_5percent_raintomorrow(data.adelaide,nb_jour,"01")

test_5percent_raintomorrow_all <- function(d,nb_jour){
  array = c()
  for (mois in c("01","02","03","04","05","06","07","08","09","10","11","12")){
      array = rbind(array,c(test_5percent_raintomorrow(d,nb_jour,mois)))
  }
  return (array)
}
a= test_5percent_raintomorrow_all(data.adelaide,nb_jour)
```

La question n'a pas vraiment de sens, tous les tests sont très largement rejetés.

# Question 3 :

On doit faire un test de comparaison entre les mois dits d'été et les mois d'hiver. Pour ça, on choisit 6 mois d'été et 6 mois d'hiver.

# Question 4 :

Maintenant que l'on a régardé comment se comportait RainTomorrow dans les différentes villes et selon les mois, on va tenter de proposer un modèle prédictif basé sur une régression logistique en modélisant la probabilité conditionnelle de pluie le lendemain selon les autres variables. 

On pose $Y = f(X_1,...,X_n)$ avec $X_1,...,X_n$ les variables explicatives de $Y$. On va estimer un échantillon $Y_1,...,Y_n$ et on a $L(Y_1,...,Y_n|B)=\prod_{i=j}^n\frac{exp(\sum_{i=1}^nB_{i,j}X_{i,j})}{1+exp(\sum_{i=1}^nB_{i,j}X_{i,j})}$
